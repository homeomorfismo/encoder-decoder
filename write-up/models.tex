\section{Finite Element setting}
\label{sec:fem}

We define a test setting for the implementation of the encoder-decoder V-cycle-like method.

Let $\Omega = [0,1]^2 \subset \mathbb{R}^2$ be a square domain.
We consider the Helmholtz equation with Dirichlet boundary conditions in $\Omega$:
\begin{subequations}
    \begin{align}
        -\Delta u + u &= 1 \quad \text{in } \Omega, \\
        u &= 0 \quad \text{on } \partial \Omega.
    \end{align}
\end{subequations}
Define the bilinear form \(a(u,v) = \int_{\Omega} \nabla u \cdot \nabla v + \int_{\Omega} uv\) and the linear form \(\ell(v) = \int_{\Omega} v\).
The weak formulation of this problem is the following:
find \(u \in \mathring{H}^1(\Omega)\) such that \(a(u,v) = \ell(v)\) for all \(v \in \mathring{H}^1(\Omega)\).

Consider the Lagrange finite elements of degree \(p\) with null trace, denoted by \(\mathring{V}_h \subset \mathring{H}^1(\Omega)\).
Morevoer, let \(a_h\) and \(\ell_h\) be the restriction of \(a\) and \(\ell\) to \(\mathring{V}_h\), respectively.
The discrete problem is to find \(u_h \in \mathring{V}_h\) such that \(a_h(u_h,v_h) = \ell_h(v_h)\) for all \(v_h \in \mathring{V}_h\).
It is well-known the stability and convergence of the Galerkin method for this problem.

Computationaly, it is convenient to consider the Lagrange finite elements of degree \(p\), denoted by \(V_h \subset H^1(\Omega)\).
We abuse notation and denote by \(a_h\) and \(\ell_h\) the restriction of \(a\) and \(\ell\) to \(V_h\), respectively.
Notice we can decompose \(V_h = \mathring{V}_h \oplus V_h^\partial\).
Now, let \( N = \dim(V_h) \) the number of degrees of freedom.
Upon a choice of basis functions \( \{ \phi_i \}_{i=1}^{N} \) for \(V_h\), we can write the finite element solution as \(u_h = \sum_{i=1}^{N} u_i \phi_i\).
Define the stiffness matrix \(\A = (A_{ij}) \in \mathbb{R}^{N \times N}\) and the load vector \(\mathbf{b} = (b_i) \in \mathbb{R}^{N}\) such that \(A_{ij} = a_h(\phi_j, \phi_i)\) and \(b_i = \ell_h(\phi_i)\).
The linear system is then \(\A \uu = \mathbf{b}\), where \(\uu \in \mathbb{R}^{N}\) is the vector of coefficients of the finite element solution, i.e., \(\uu = (u_1, \ldots, u_N)\) and \(u_h = \sum_{i=1}^{N} u_i \phi_i\), \emph{provided} \(u_h \in \mathring{V}_h\), i.e., the trace of \(u_h\) is null.

We highligh that the matrix \(\A\) is symmetric and positive definite.
Moreover, the matrix \(\A\) and the vector \(\mathbf{b}\) (up to reordering of the basis functions) can be written as
\begin{align*}
    \A & = \begin{bmatrix}
        \A_{\circ, \circ} & \A_{\circ, \partial} \\
        \A_{\partial, \circ} & \A_{\partial, \partial}
    \end{bmatrix}, &
    \uu & = \begin{bmatrix}
        \uu_{\circ} \\
        \uu_{\partial}
    \end{bmatrix}, &
    \bb & = \begin{bmatrix}
        \bb_{\circ} \\
        \bb_{\partial}
    \end{bmatrix},
\end{align*}
When the boundary conditions are prescribed and given by \(\uu_{\partial}\), we can rewrite the linear system as
\begin{equation}
    \begin{bmatrix}
        \A_{\circ, \circ} & 0 \\
        0 & \operatorname{diag}(\A_{\partial, \partial})
    \end{bmatrix}
    \begin{bmatrix}
        \uu_{\circ} \\
        \uu_{\partial}
    \end{bmatrix}
    =
    \begin{bmatrix}
        \mathbf{b}_{\circ} - \A_{\circ, \partial} \uu_{\partial} \\
        \operatorname{diag}(\A_{\partial, \partial}) \uu_{\partial}
    \end{bmatrix},
\end{equation}
[TODO]
This detail is relevant for the generation of the training data (see Section~\ref{sec:data}), as for the proposed Two-level V-cycle-like methods.
For convenience, we denote
\begin{equation}
    \overline{\A} = \begin{bmatrix}
        \A_{\circ, \circ} & 0 \\
        0 & \operatorname{diag}(\A_{\partial, \partial})
    \end{bmatrix}.
\end{equation}

\section{Encoder-decoder}
\label{sec:e-d}

In this section we introduce the encoder-decoder model that we will use to develop a solver for the a finite element problem.

\subsection{Linear encoder-decoder}

We consider a single-layer linear encoder-decoder model.
We denote by \(\W \in \mathbb{R}^{N \times n}\) the encoder matrix and by \(\V \in \mathbb{R}^{n \times N}\) the decoder matrix.
The encoder-decoder model is described by the two following equations:
\begin{subequations}
    \begin{align}
        \yyc & = \W \uu, \\
        \yy & = \V \yyc,
    \end{align}
\end{subequations}
where \(\yy \in \mathbb{R}^{N}\) is the output of the model.
We denote this operation by \(\EDNN(\uu) = \yy\).

Consider \( \{ \uu^{(i)} \}_{i=1}^{m} \) a set of \(m\) samples of the finite element solution.
We propose to minimize the following loss function:
\begin{equation}
    \J(\W, \V) =
    \frac{1}{m} \sum_{i=1}^{m} \left\| \uu^{(i)} - \EDNN(\uu^{(i)}) \right\|_2^2 +
    \lambda \R(\W, \V),
\end{equation}
where \(\lambda > 0\) is a regularization parameter and \(\R(\W, \V)\) is a regularization term.

% Reminder: W:RN to Rnc is encoder, V:Rnc to RN is decoder.
Proposed regularization terms are:
\begin{subequations}
    \label{eq:regularization}
    \begin{align}
        \R_1(\W, \V) &
        = \norm{\W }_{\ell^1} + \norm{\V }_{\ell^1}
        + \norm{\W \W^T - \Ic }_{\ell^2} + \norm{\V \V^T - \Ic }_{\ell^2}
        + \norm{\W \V - \Ic }_{\ell^2}, \\
        \R_2(\W, \V) &
        = \norm{\W }_{\ell^1} + \norm{\V }_{\ell^1}
        + \norm{\W \V - \Ic }_{\ell^2}.
    \end{align}
\end{subequations}

\subsection{Multigrid-like encoder-decoder}

We add a extra layer to the basic linear encoder-decoder model.
This layer consists of an application of the original operator from the Galerkin problem.
We denote by \(\W \in \mathbb{R}^{N \times n}\) the encoder matrix and by \(\V \in \mathbb{R}^{n \times N}\) the decoder matrix.
The encoder-decoder model is described by the three following equations:
\begin{subequations}
    \begin{align}
        \yyc & = \W \uu, \\
        \yyf & = \V \yyc, \\
        \yy & = \A \yyf,
    \end{align}
\end{subequations}
where \(\yy \in \mathbb{R}^{N}\) is the output of the model.
We denote this operation by \(\EDNNMG(\uu) = \yy\).

Consider \( \{ \uu^{(i)} \}_{i=1}^{m} \) a set of \(m\) samples of the finite element solution, and define the residuals \(\mathbf{r}^{(i)} = \A \uu^{(i)}\).
We propose to minimize the following loss function:
\begin{equation}
    \J(\W, \V) =
    \sum_{i=1}^{m} \norm{\V^T( \EDNNMG(\uu^{(i)}) - \mathbf{r}^{(i)} ) }_2^2 +
    \lambda \R(\W, \V),
\end{equation}
where \(\lambda > 0\) is a regularization parameter and \(\R(\W, \V)\) is a regularization term.
We use the same regularization terms as in the linear encoder-decoder model.
See~\eqref{eq:regularization}.

\section{Data generation}
\label{sec:data}

In Section~\ref{sec:fem} we have described a simple example of a finite element problem, which we will use to generate the training data.
In Section~\ref{sec:e-d} we have described the encoder-decoder models that we will use to develop a solver for the finite element problem.
Now we require data to train the models.

The main idea is the following: given a set of vectors \( \{ \zz^{(i)} \}_{i=1}^{m} \), and a convergent symmetric smoother \( \M\) for the operator \(\A\), we can generate \( \uu^{(i)} = (\I - \M^{-1} \A) \zz^{(i)} \).
We use a symmetrized Gauss-Seidel smoother as the convergent smoother.
Notice that we do not keep the boundary conditions in the training data.

[TODO]
We consider two types of vectors: randomly generated vectors with entries in \([0,1]\),
% vectors with entries in \(\{0,1\}\),
and interpolation of smooth functions.
The interpolated functions are of the form \(u_i(x,y) = \sin(2 \pi \alpha_i x) \sin(2 \pi \beta_i y) + \gamma_i\), where \(\alpha_i, \beta_i, \gamma_i\) are randomly generated numbers in \([0,1]\).

We employ a symmetrized Gauss-Seidel smoother \(\M_\SGS\) to generate the training data.
The smoother is applied until the residual is smaller than a given tolerance \(\tol{\SGS} = 0.1\) or until a maximum number of iterations \(\niter{\SGS} = 1.0 \times 10^5\) is reached.

\section{Implementation and numerical experiments}

We assemble and train the models described in Section~\ref{sec:e-d} using the framework described in Section~\ref{sec:fem}.
NGSolve~\cite{schoberl2014c++} is used to solve the finite element problem and to generate the training data.
Keras~\cite{chollet2015keras} is used to implement the models and to train them.
In particular, we use the Adam optimizer~\cite{kingma2014adam} for training.

\subsection{Parameters for data generation and training}

[TODO]
We consider the following parameters for the models.
We discretize the domain \(\Omega = [0,1]^2\) with \(\maxh = 5.0 \times 10^{-2}\).
We consider lowest-order Lagrange finite elements, i.e., \(p = 1\).
Regarding the data generation, we consider \(m_\mathrm{rnd} = 1000\) randomly generated vectors and \(m_\mathrm{smo} = 1000\) vectors obtained by interpolation of smooth functions, as described in Section~\ref{sec:data}.

[TODO]
The basic linear encoder-decoder model is trained is defined in the following way.
We consider a compression factor of \(\compressionfactor = 5.0\).
This means that the number of neurons in the hidden layer is \(n = \lfloor N/\compressionfactor \rfloor\).
We employ a learning rate of \(\learningrate = 0.001\), we train the model for \(n_\mathrm{epochs} = 100\), and we use a batch size of \(n_\mathrm{batch} = 100\).
We initialize the weights with different strategies, we describe these strategies in Table~\ref{tab:weights}.
See~\cite{chollet2015keras, glorot2010understanding} for more details.
We use the regularization term~\eqref{eq:regularization} with \(\lambda = 1.0 \times 10^{-3}\).

Due to the nature of the encoder-decoder model, we may not get sparse matrices \(\W\) and \(\V\).
This is because the model is not required to be sparse, and the regularization terms~\eqref{eq:regularization} may not be enough to enforce sparsity.
Therefore, we consider additional matrices \(\W_\mathrm{t}\) and \(\V_\mathrm{t}\) defined by zeroing out entries of \(\W\) and \(\V\) that are smaller than a given threshold \(\tol{\mathrm{t}} = 1.0 \times 10^{-2}\).

\begin{table}[htbp]
    \centering
    \begin{tabular}{c|cccc}
        \diagbox{Encoder}{Decoder} & \textsc{Glorot uniform} & \textsc{Identity} & \textsc{Zeros} & \textsc{Ones} \\
        \hline
        \textsc{Glorot uniform} & \checkmark & \checkmark & \checkmark & \checkmark \\
        \textsc{Identity} & \checkmark & \checkmark & \checkmark & \checkmark \\
        \textsc{Zeros} & \checkmark & \checkmark & \checkmark & \checkmark \\
        \textsc{Ones} & \checkmark & \checkmark & \checkmark & \checkmark \\
    \end{tabular}
    \caption{Initialization strategies for the weights.}
    \label{tab:weights}
\end{table}

\subsection{Coarse operator implementation}

After training, the matrices \(\W, \V, \W_\mathrm{t}, \V_\mathrm{t}\) are available.
We define the following coarse operators:
\begin{subequations}
    \begin{align}
        \A_{\mathrm{c}, \mathrm{d}} & = \V^\mathsf{T} \overline{\A} \V, \\
        \A_{\mathrm{c}, \mathrm{e}} & = \W \overline{\A} \W^\mathsf{T}, \\
        \A_{\mathrm{c}, \mathrm{dt}} & = \V_\mathrm{t}^\mathsf{T} \overline{\A} \V_\mathrm{t}, \\
        \A_{\mathrm{c}, \mathrm{et}} & = \W_\mathrm{t} \overline{\A} \W_\mathrm{t}^\mathsf{T}.
    \end{align}
\end{subequations}


\subsection{Algorithms}

In the following we describe the algorithms used to generate the training data and to train the models.

\begin{algorithm}[H]
    \caption{Data generation for a finite element problem.}
    \label{alg:data-gen}
    % \SetKwFunction{InitializeVector}{InitializeVector}

    \KwIn{
        A PSD operator \(\A\),
        an \(\A\)-convergent smoother \(\M_\bullet\),
        % a set of (null) vectors \(\{ \zz^{(i)} \}_{i=1}^{m}\),
        a number of desired samples \(m\),
        a tolerance \(\tol{\bullet}\),
        a maximum number of iterations \(\niter{\bullet}\).
        % Here, \(\bullet \in \{ \SGS, \ell^1\}\).
    }
    \KwResult{
        A set of vectors \(\{ \zz^{(i)} \}_{i=1}^{m}\).
    }
    \For{\(i = 1\) \KwTo \(m\)}{
        \(\zz^{(i)} \gets \textsc{InitializeVector}\) \;
        \(\bb^{(i)} \gets \A \zz^{(i)}\) \;
        \(\ee^{(i)} \gets \textsc{Solve}(\M_\bullet, \bb^{(i)}; \tol{\bullet}, \niter{\bullet})\) \;
        \(\zz^{(i)} \gets \zz^{(i)} - \ee^{(i)}\) \;
    }
\end{algorithm}

\begin{algorithm}[H]
    \caption{Training a linear encoder-decoder model.}
    \label{alg:train-lin}
    \KwIn{
        A set of vectors \(\{ \zz^{(i)} \}_{i=1}^{m}\),
        a compression factor \(\compressionfactor\),
        a learning rate \(\learningrate\),
        a number of epochs \(n_\mathrm{epochs}\),
        a batch size \(n_\mathrm{batch}\),
        a threshold \(\tol{\mathrm{t}}\).
    }
    \KwResult{
        Encoder matrix \(\W\),
        Decoder matrix \(\V\),
        Encoder matrix with thresholding \(\W_\mathrm{t}\),
        Decoder matrix with thresholding \(\V_\mathrm{t}\).
    }
    \(\W, \V \gets \textsc{InitializeWeights}\) \;
    \(\W, \V \gets \textsc{Train}(\W, \V; \{ \zz^{(i)} \}_{i=1}^{m}, \compressionfactor, \learningrate, n_\mathrm{epochs}, n_\mathrm{batch})\) \;
    \(\W_\mathrm{t}, \V_\mathrm{t} \gets \textsc{Threshold}(\W, \V; \tol{\mathrm{t}})\) \;
\end{algorithm}

\begin{algorithm}[H]
    \caption{A two-level method}
    \label{alg:two-level}
    \KwIn{
        A vector \(\uu\),
        Operator \(\A\),
        Smoother \(\M\),
        Projection matrix \(\PP\in\{ \W, \W_\mathrm{t}, \V^\mathsf{T}, \V_\mathrm{t}^\mathsf{T} \}\),
        Right-hand side \(\bb\),
    }
    \KwResult{
        A vector \(\yy\).
    }
    \(\uu \gets \textsc{InitializateVector}\) \;
    \(\rr \gets \bb - \A \uu\) \;
    % Pre-smoothing
    \(\ee \gets \textsc{Solve}(\M, \rr; \tol{\FGS}, \niter{\FGS})\) \;
    \(\uu \gets \uu + \ee\) \;
    % Coarse grid correction
    \(\rr \gets \bb - \A \uu\) \;
    \(\rrc \gets \PP^\mathsf{T} \rr\) \;
    \(\Ac \gets \PP^\mathsf{T} \A \PP\) \;
    \(\yyc \gets \textsc{Solve}( \Ac, \rrc; \tol{}, \niter{})\) \;
    \(\yyf \gets \PP \yyc\) \;
    \(\uu \gets \uu + \yyf\) \;
    % Post-smoothing
    \(\rr \gets \bb - \A \uu\) \;
    \(\ee \gets \textsc{Solve}(\M^\mathsf{T}, \rr; \tol{\BGS}, \niter{\BGS})\) \;
    \(\uu \gets \uu + \ee\) \;
\end{algorithm}
