\section{Finite Element setting}
\label{sec:fem}

We define a test setting for the implementation of the encoder-decoder V-cycle-like method.

Let $\Omega = [0,1]^2 \subset \mathbb{R}^2$ be a square domain.
We consider the Helmholtz equation with Dirichlet boundary conditions in $\Omega$:
\begin{subequations}
    \begin{align}
        -\Delta u + u &= 1 \quad \text{in } \Omega, \\
        u &= 0 \quad \text{on } \partial \Omega.
    \end{align}
\end{subequations}
Define the bilinear form \(a(u,v) = \int_{\Omega} \nabla u \cdot \nabla v + \int_{\Omega} uv\) and the linear form \(\ell(v) = \int_{\Omega} v\).
The weak formulation of this problem is the following:
find \(u \in \mathring{H}^1(\Omega)\) such that \(a(u,v) = \ell(v)\) for all \(v \in \mathring{H}^1(\Omega)\).

Consider the Lagrange finite elements of degree \(p\) with null trace, denoted by \(\mathring{V}_h \subset \mathring{H}^1(\Omega)\).
Morevoer, let \(a_h\) and \(\ell_h\) be the restriction of \(a\) and \(\ell\) to \(\mathring{V}_h\), respectively.
The discrete problem is to find \(u_h \in \mathring{V}_h\) such that \(a_h(u_h,v) = \ell_h(v)\) for all \(v \in \mathring{V}_h\).
It is well-known the stability and convergence of the Galerkin method for this problem.

Computationaly, it is convenient to consider the Lagrange finite elements of degree \(p\), denoted by \(V_h \subset H^1(\Omega)\).
We abuse notation and denote by \(a_h\) and \(\ell_h\) the restriction of \(a\) and \(\ell\) to \(V_h\), respectively.
Notice we can decompose \(V_h = \mathring{V}_h \oplus V_h^\partial\).
Now, let \( N = \dim(V_h) \) the number of degrees of freedom.
Upon a choice of basis functions \( \{ \phi_i \}_{i=1}^{N} \) for \(V_h\), we can write the finite element solution as \(u_h = \sum_{i=1}^{N} u_i \phi_i\).
Define the stiffness matrix \(\A = (A_{ij}) \in \mathbb{R}^{N \times N}\) and the load vector \(\mathbf{b} = (b_i) \in \mathbb{R}^{N}\) such that \(A_{ij} = a_h(\phi_j, \phi_i)\) and \(b_i = \ell_h(\phi_i)\).
The linear system is then \(\A \uu = \mathbf{b}\), where \(\uu \in \mathbb{R}^{N}\) is the vector of coefficients of the finite element solution, i.e., \(\uu = (u_1, \ldots, u_N)\) and \(u_h = \sum_{i=1}^{N} u_i \phi_i\), \emph{provided} \(u_h \in \mathring{V}_h\), i.e., the trace of \(u_h\) is null.

We highligh that the matrix \(\A\) is symmetric and positive definite.
Moreover, the matrix \(\A\) and the vector \(\mathbf{b}\) (up to reordering of the basis functions) can be written as
\begin{align*}
    \A & = \begin{bmatrix}
        \A_{\circ, \circ} & \A_{\circ, \partial} \\
        \A_{\partial, \circ} & \A_{\partial, \partial}
    \end{bmatrix}, &
    \mathbf{b} & = \begin{bmatrix}
        \mathbf{b}_{\circ} \\
        \mathbf{b}_{\partial}
    \end{bmatrix},
\end{align*}
When the boundary conditions are prescribed and given by \(\uu_{\partial}\), we can rewrite the linear system as
\begin{equation}
    \begin{bmatrix}
        \A_{\circ, \circ} & 0 \\
        0 & \operatorname{diag}(\A_{\partial, \partial})
    \end{bmatrix}
    \begin{bmatrix}
        \uu_{\circ} \\
        \uu_{\partial}
    \end{bmatrix}
    =
    \begin{bmatrix}
        \mathbf{b}_{\circ} - \A_{\circ, \partial} \uu_{\partial} \\
        \operatorname{diag}(\A_{\partial, \partial}) \uu_{\partial}
    \end{bmatrix},
\end{equation}
This detail is relevant for the generation of the training data.

\section{Encoder-decoder}
\label{sec:e-d}

In this section we introduce the encoder-decoder model that we will use to develop a solver for the a finite element problem.

\subsection{Linear encoder-decoder}

We consider a single-layer linear encoder-decoder model.
We denote by \(\W \in \mathbb{R}^{N \times n}\) the encoder matrix and by \(\V \in \mathbb{R}^{n \times N}\) the decoder matrix.
The encoder-decoder model is desciber by the two following equations:
\begin{subequations}
    \begin{align}
        \yyc & = \W \uu, \\
        \yy & = \V \yyc,
    \end{align}
\end{subequations}
where \(\yy \in \mathbb{R}^{N}\) is the output of the model.
We denote this operation by \(\EDNN(\uu) = \yy\).

Consider \( \{ \uu^{(i)} \}_{i=1}^{m} \) a set of \(m\) samples of the finite element solution.
We propose to minimize the following loss function:
\begin{equation}
    \J(\W, \V) =
    \frac{1}{m} \sum_{i=1}^{m} \left\| \uu^{(i)} - \EDNN(\uu^{(i)}) \right\|_2^2 +
    \lambda \R(\W, \V),
\end{equation}
where \(\lambda > 0\) is a regularization parameter and \(\R(\W, \V)\) is a regularization term.

Proposed regularization terms are:
\begin{subequations}
    \begin{align}
        \R_1(\W, \V) &
        = \norm{\W }_{\ell^1} + \norm{\V }_{\ell^1}
        + \norm{\W \W^T - \I_\mathrm{c} }_{\ell^2}
        + \norm{\W \V - \I_\mathrm{c} }_{\ell^2}
        + \norm{\V - \W^T }_{\ell^2}, \\
        \R_2(\W, \V) &
        = \norm{\W }_{\ell^1} + \norm{\V }_{\ell^1}
        + \norm{\V \W - \I }_{\ell^1}.
    \end{align}
\end{subequations}

\subsection{Multigrid-like encoder-decoder}

We add a extra layer to the basic linear encoder-decoder model.
This layer consists of an application of the original operator from the Galerkin problem.
We denote by \(\W \in \mathbb{R}^{N \times n}\) the encoder matrix and by \(\V \in \mathbb{R}^{n \times N}\) the decoder matrix.
The encoder-decoder model is desciber by the three following equations:
\begin{subequations}
    \begin{align}
        \yyc & = \W \uu, \\
        \yyf & = \V \yyc, \\
        \yy & = \A \yyf,
    \end{align}
\end{subequations}
where \(\yy \in \mathbb{R}^{N}\) is the output of the model.
We denote this operation by \(\EDNNMG(\uu) = \yy\).

Consider \( \{ \uu^{(i)} \}_{i=1}^{m} \) a set of \(m\) samples of the finite element solution, and define the residuals \(\mathbf{r}^{(i)} = \A \uu^{(i)}\).
We propose to minimize the following loss function:
\begin{equation}
    \J(\W, \V) =
    \sum_{i=1}^{m} \norm{\V^T( \EDNNMG(\uu^{(i)}) - \mathbf{r}^{(i)} ) }_2^2 +
    \lambda \R(\W, \V),
\end{equation}
where \(\lambda > 0\) is a regularization parameter and \(\R(\W, \V)\) is a regularization term.

Proposed regularization terms are:
\begin{subequations}
    \begin{align}
        \R_1(\W, \V) &
        = \norm{\W }_{\ell^1} + \norm{\V }_{\ell^1}
        + \norm{\W \W^T - \Ic }_{\ell^1}
        + \norm{\V^T \V - \Ic }_{\ell^1}, \\
        \R_2(\W, \V) &
        = \norm{\W }_{\ell^1} + \norm{\V }_{\ell^1}
        + \norm{\V \W - \I }_{\ell^1}.
    \end{align}
\end{subequations}

\section{Data generation}

In Section~\ref{sec:fem} we have described a simple example of a finite element problem, which we will use to generate the training data.
In Section~\ref{sec:e-d} we have described the encoder-decoder models that we will use to develop a solver for the finite element problem.
Now we require data to train the models.

The main idea is the following: given a set of vectors \( \{ \zz^{(i)} \}_{i=1}^{m} \), and a convergent smoother \( \M\) for the operator \(\A\), we can generate \( \uu^{(i)} = (\I - \M^{-1} \A) \zz^{(i)} \).
We use a symmetrized Gauss-Seidel smoother as the convergent smoother.
Notice that we do not keep the boundary conditions in the training data.

[TODO]
We consider two types of vectors, say randomly generated vectors with entries in \([0,1]\), vectors with entries in \(\{0,1\}\), and interpolation of smooth functions.
The interpolated functions are of the form \(u_i(x,y) = \sin(2 \pi \alpha_i x) \sin(2 \pi \beta_i y) + \gamma_i\), where \(\alpha_i, \beta_i, \gamma_i\) are randomly generated numbers in \([0,1]\).

\section{Implementation and numerical experiments}

We assemble and train the models described in Section~\ref{sec:e-d} using the framework described in Section~\ref{sec:model}.
NGSolve~\cite{schoberl2014c++} is used to solve the finite element problem and to generate the training data.
Keras~\cite{chollet2015keras} is used to implement the models and to train them.
In particular, we use the Adam optimizer~\cite{kingma2014adam} for training.
